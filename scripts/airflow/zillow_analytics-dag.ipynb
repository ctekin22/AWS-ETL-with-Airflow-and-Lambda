{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from datetime import timedelta, datetime\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n",
    "from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\n",
    "import json \n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define default_args with metadata and retry policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 8, 1),  # adjust date as needed\n",
    "    'email': ['your-email@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(seconds=15),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You create a Python function that uses requests to call the RapidAPI endpoint.\n",
    "- In your Python function, add a parameter to accept the context, usually called **kwargs.\n",
    "- Then you can access things like kwargs['execution_date'], kwargs['task_instance'], etc.\n",
    "- This requires passing op_kwargs (or just kwargs in Airflow 2.x).\n",
    "- Write the API response to a JSON file in your EC2 instance inside your DAG’s Python function.\n",
    "- Return a list of output file paths and file names from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_now_string = now.strftime(\"%d%m%Y%H%M%S\")  # Use a dt_string string for filenames\n",
    "\n",
    "# Define S3 bucket for transformed data\n",
    "s3_bucket = 'transformed-json-data-to-csv-bucket'\n",
    "\n",
    "# Define your Python callable for the task\n",
    "def extract_zillow_data(**kwargs):\n",
    "    url =kwargs['url']\n",
    "    headers = kwargs['headers']\n",
    "    querystring = kwargs['querystring']\n",
    "    dt_string = kwargs['date_string']\n",
    "\n",
    "    #return headers\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Specify the output file path\n",
    "    # Use a dt_string string for filenames\n",
    "    output_file_path = f\"/home/ubuntu/response_data_{dt_string}.json\"\n",
    "    file_str = f\"response_data_{dt_string}.csv\"\n",
    "\n",
    "    # Write the JSON response to a file\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(response_data, output_file, indent=4)\n",
    "    output_list = [output_file_path,  file_str]\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create extract_data_from_rapidapi() operator\n",
    "- Instantiate the DAG with ID, schedule, description, etc.\n",
    "- Wrap that function in a PythonOperator so Airflow can schedule and run it.\n",
    "- The DAG runs once daily (or however you want to schedule it).\n",
    "- Use the absolute path to the JSON file (/home/ubuntu/airflow/config_api.json).\n",
    "- The Python json.load() reads your API keys into the config dictionary.\n",
    "- Your Python callable extract_data_from_rapidapi() uses those keys from config.\n",
    "- The task_id in PythonOperator must be unique within the DAG.\n",
    "\n",
    "### Create S3KeySensor operator\n",
    "The S3KeySensor is a built-in Airflow sensor operator provided by the Amazon provider package. Its job is to continuously check if a specific file (key) exists inside an S3 bucket before allowing the DAG to proceed to the next task—like loading into Redshift. It’s a critical part of event-driven data workflows, especially when working with eventual consistency in S3 or asynchronous processing like Lambda.\n",
    "\n",
    "You need an Airflow connection for each AWS service you interact with via Airflow operators or Python code that uses boto3, unless you rely on IAM roles or environment-based authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "with open('/home/ubuntu/airflow/config_api.json', 'r') as config_file:\n",
    "    api_host_key = json.load(config_file)\n",
    "\n",
    "# Instantiate the DAG\n",
    "with DAG(\n",
    "    dag_id='zillow_analytics_dag',\n",
    "    default_args=default_args,\n",
    "    description='A DAG to extract data from Zero API via RapidAPI',\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    \n",
    "    # Create the PythonOperator task\n",
    "    # PythonOperator using boto3 needs Airflow Connection\n",
    "    # Do You Need an Airflow Connection? That depends on what’s inside your extract_zillow_data() function:\n",
    "    # You do need an Airflow Connection if: Inside your function, you use boto3 (AWS SDK for Python) to upload to S3 or access other AWS services\n",
    "    extract_zillow_data_var = PythonOperator(\n",
    "    task_id='tsk_extract_zillow_data_var',\n",
    "    python_callable=extract_zillow_data,\n",
    "    op_kwargs={'url': \"https://zillow56.p.rapidapi.com/search_polygon\", 'querystring': {\"polygon\":\"34.03959576441558 -118.50636536779786,34.0418716916327 -118.50276047888184,34.042440663894304 -118.49846894445801,34.04201393505594 -118.49417741003418,34.04087598099002 -118.4897142142334,34.03945351693672 -118.48525101843262,34.03788877892429 -118.48095948400879,34.03618175908096 -118.47683961096192,34.034190192514366 -118.47271973791504,34.031629538228394 -118.46962983312989,34.02835747861639 -118.4677415579834,34.02465847668084 -118.46671158972168,34.02081703478521 -118.46636826696778,34.01697541902413 -118.46636826696778,34.01341821237762 -118.4673982352295,34.011283816847104 -118.47100312414551,34.01057233974687 -118.47563798132325,34.01043004361143 -118.47992951574707,34.01071463564384 -118.48439271154786,34.01156840601794 -118.48868424597168,34.01270675316253 -118.49297578039551,34.01398737545716 -118.49709565344239,34.01555255425154 -118.50104386511231,34.01754455825562 -118.50464875402832,34.02039019717532 -118.50756699743653,34.02352028980117 -118.50962693395996,34.02707707311613 -118.51065690222168,34.03063370735633 -118.50997025671387,34.034190192514366 -118.5091119498291,34.03774652858273 -118.50825364294434,34.03959576441558 -118.50636536779786\",\"output\":\"json\",\"status\":\"forSale\",\"sortSelection\":\"priorityscore\",\"listing_type\":\"by_agent\",\"doz\":\"any\"}, 'headers':api_host_key, 'date_string':dt_now_string}\n",
    "    )\n",
    "\n",
    "    # Uses environment credentials or IAM roles already configured\n",
    "    # Running commands directly on the host (EC2)\n",
    "    # Airflow is not involved in the authentication — Bash just calls a shell command.\n",
    "    load_to_s3 = BashOperator(\n",
    "       task_id = 'tsk_load_to_s3',\n",
    "       bash_command = 'aws s3 mv {{ti.xcom_pull(\"tsk_extract_zillow_data_var\")[0]}} s3://zillow-landing-stage/',\n",
    "    )\n",
    "\n",
    "    # This does require an Airflow Connection, because it uses the Python boto3 SDK under the hood,\n",
    "    # Airflow needs to know how to authenticate to AWS within Python\n",
    "    is_file_in_s3_available = S3KeySensor(\n",
    "        task_id = 'tsk_is_file_in_s3_available',\n",
    "        bucket_key = '{{ti.xcom_pull(\"tsk_extract_zillow_data_var\")[1]}}',\n",
    "        bucket_name = s3_bucket,\n",
    "        aws_conn_id = 'aws_s3_conn', # will create this in Airflow\n",
    "        wildcard_match = False,\n",
    "        timeout = 120,\n",
    "        poke_interval=5,\n",
    "    )\n",
    "   \n",
    "   # Both aws_conn_id and redshift_conn_id are required because Airflow needs to:\n",
    "   # Connect to AWS (for S3)\n",
    "   # Connect to Redshift (to run the COPY command)\n",
    "    \n",
    "    transfer_s3_to_redshift = S3ToRedshiftOperator(\n",
    "        task_id = 'tsk_transfer_s3_to_redshift',\n",
    "        aws_conn_id = 'aws_s3_conn', \n",
    "        redshift_conn_id = 'redshift_conn_id', # will create this in Airflow\n",
    "        s3_bucket = s3_bucket,\n",
    "        s3_key = '{{ti.xcom_pull(\"tsk_extract_zillow_data_var\")[1]}}',\n",
    "        schema = 'public',\n",
    "        table = 'zillowdata',\n",
    "        copy_options=['CSV', 'IGNOREHEADER 1', 'EMPTYASNULL', 'BLANKSASNULL'],\n",
    "    )\n",
    "\n",
    "    # Set task dependency so that upload runs after extraction\n",
    "    extract_zillow_data_var >> load_to_s3 >> is_file_in_s3_available >>  transfer_s3_to_redshift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
