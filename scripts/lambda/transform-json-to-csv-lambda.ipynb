{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "\n",
    "    s3_client = boto3.client('s3')\n",
    "    source_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    object_key =  event['Records'][0]['s3']['object']['key']\n",
    "    #print(source_bucket) # these print statements will show on CloudWatch logs\n",
    "    #print(object_key)\n",
    "\n",
    "    target_bucket = 'transformed-json-data-to-csv-bucket'\n",
    "    target_file_name = object_key[:-5] # excludes '.json' file extention from file name, will keep file name same but adding csv extetion\n",
    "    #print(target_file_name)\n",
    "\n",
    "    waiter = s3_client.get_waiter('object_exists')\n",
    "    waiter.wait(Bucket=source_bucket, Key=object_key)\n",
    "\n",
    "    response = s3_client.get_object(Bucket=source_bucket, Key=object_key)\n",
    "    #print(response)\n",
    "\n",
    "    # response is a dictionary that contains:   \n",
    "    # Body: a streaming object containing the file's content\n",
    "    # ContentLength, ContentType, and metadata\n",
    "    # It called \"Streaming\" because, it does not download the entire file into memory immediately\n",
    "    # It streams the content in chunks from S3. This is more memory-efficient â€” great for large files!\n",
    "    data = response['Body'] # StreamingBody object\n",
    "    #print(data)\n",
    "\n",
    "    # .read() reads the entire byte content of the file from the stream.\n",
    "    # .decode('utf-8') converts the bytes into a string.\n",
    "    data = response['Body'].read().decode('utf-8')\n",
    "    #print(data)\n",
    "\n",
    "    # Parses the JSON string into a Python object (usually a dict).\n",
    "    data = json.loads(data)\n",
    "    #print(data)\n",
    "\n",
    "    f =[]\n",
    "    for i in data[\"results\"]:\n",
    "        f.append(i)\n",
    "    df = pd.DataFrame(f)\n",
    "    selected_columns = ['bathrooms', 'bedrooms', 'city', 'homeStatus','homeType','livingArea','price','rentZestimate','zipcode']\n",
    "\n",
    "    df = df[selected_columns]\n",
    "    #print(df)\n",
    "\n",
    "    # Convert DataFrame to CSV format\n",
    "    csv_data = df.to_csv(index=False)\n",
    "\n",
    "    # Upload CSV to S3\n",
    "    bucket_name = target_bucket\n",
    "    object_key = f\"{target_file_name}.csv\"\n",
    "    s3_client.put_object(Bucket= bucket_name, Key = object_key, Body=csv_data)\n",
    "\n",
    "    return{\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('CSV conversion and S3 upload completed succrssfully')\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
