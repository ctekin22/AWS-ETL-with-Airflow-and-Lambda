{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json: Standard Python library to handle JSON formatting.\n",
    "\n",
    "boto3: AWS SDK for Python. Used to interact with AWS services (like S3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Creates an S3 client so the Lambda function can perform S3 operations (e.g., copy files).\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# This is the main entry point for any Lambda function.\n",
    "# event contains details of what triggered the Lambda (in this case, an S3 file upload).\n",
    "# context provides runtime info (not used here)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # Extracts the source bucket name and file key (file name) from the event JSON payload.\n",
    "    # This event structure is sent by S3 when a new file is uploaded.\n",
    "    source_bucket  = event['Records'][0]['s3']['bucket']['name']\n",
    "    object_key = event['Records'][0]['s3']['object']['key']\n",
    "\n",
    "    # Sets the target bucket (where you want to copy the file).\n",
    "    # Prepares a copy source dictionary needed by the copy_object() call.\n",
    "    target_bucket = 'copy-of-raw-json-intermediate-bucket'\n",
    "    copy_source = {'Bucket': source_bucket, 'Key': object_key}\n",
    "\n",
    "    # Uses an S3 waiter to pause execution until the uploaded file is fully available.\n",
    "    # This prevents copying a file that's still being uploaded (especially important for large files)\n",
    "    waiter = s3_client.get_waiter('object_exists')\n",
    "    waiter.wait(Bucket=source_bucket, Key=object_key)\n",
    "\n",
    "    # Executes the copy operation from the source to the target S3 bucket.\n",
    "    s3_client.copy_object(Bucket=target_bucket, Key=object_key, CopySource=copy_source)\n",
    "\n",
    "    # Returns a success response for debugging and logging.\n",
    "    # Not essential for the S3 trigger to work, but helpful for tracing in CloudWatch.\n",
    "    return{\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Copy completed successfully')\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
